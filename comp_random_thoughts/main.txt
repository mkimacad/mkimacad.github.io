header:Don't try approximating activation functions with RELU
I tried it. It doesn't work.
Why did I try it? Well, so that I can do forward high-order derivative passes.
It doesn't work. Making neural networks learn high-order derivative heuristics and somehow expect functions to extrapolate well? Doesn't work.
And a better way exists.
header:MoE and prompt engineering (July 10, 2025 - ?)
(To be updated)
header:So-called top 50 interview questions about LLM (July 10, 2025 - ?)
(To be updated)
header:Can we remain optimistic about LLM as before? (very shallow, June 17, 2025)
LLMs no longer scale well relative to additional resource.
Also, while additional resource, better performance but more hallucinations.
Chain-of-Thoughts (CoTs) not always good. Sometimes degrades performances.
More computation time involved, degradation in performance as well: trade-off in CoT reasoning.
Would self-reinforcement learning help? Maybe, but likely not far enough to resolve significant issues.
Basically, LLMs continue to have difficult time dealing with what they consider as unfamiliar.
Illusions of complexity. That LLMs solve familiar complex tasks gives us impressions that LLMs should eventually work.
That may not be the case.
