header:History (dates-wise)
v1 code (July 10-11, 2025)
Updating further soon
header:Codes
The initial v1 code can be viewed from the below repo viewer, located at codes/extrapolation.py. (July 10-11, 2025)
header:On the [-1,1] "constraint" of neural networks - is extrapolation on neural networks really possible?
Assume, for simplification, one-dimensional input domain.
If you think about it, if we can extract information well on [-1,1], then as long as the interpolating function is smooth, we know everything.
But neural networks do not behave like this. Outside of the interpolating domain where sample data are available, they behave poorly.
This is largely due to the nature of activation functions.
The worst case is RELU activation, where we high-order derivatives are all zero. So neural networks with RELU are essentially interpolating memorizers.
No RELU for extrapolation.
But what about other smooth activation functions? Can we do well by obtaining high-order derivative data?
Problem: high-order differentiation of neural nets is quite expensive. And we are doing additional differentiations on the loss function for derivative matching.
You may consider finite difference methods, but they are bound to different numerical issues.
Neural nets can only approximate some smooth function, so there will be some cases where derivatives are significantly wrong for correct computation of high-order derivatives.
So we need different ways to have good extrapolation behaviors. Expanding the sample input domain is not an option, as aforementioned.
header:Why not polynomials for extrapolation?
Polynomials do provide good bounded extrapolation performance for sufficiently low-derivative signals.
Nevertheless, they unnecessarily lose valuable high-order derivative information. Furthermore, it is not like we cannot extrapolate forever.
And there are cases where polynomials are clearly unsuited, especially in high-dimensional input high-dimensional output cases, along with non-linear optimization cases.
header:How do we "preserve" high-order derivative information of a signal?
Directly providing high-order derivatives so that neural networks match to them is infeasible.
Instead, we work in complex-valued domain and expand to complex-valued output. We move away from real analysis to complex analysis.
header:Holomorphic function
If we assume that our signals are holomorphic, then a significantly constraining constraint can be provided - Cauchy-Riemann equations.
So our neural nets evaluate and punish Cauchy-Rimeann equation residuals so that neural networks properly learn smooth interpolation and extrapolation.
This is much better than penalizing neural nets for non-smoothness measures directly. It is samples that tell us how to be (more) smooth (or not), not interpolated results.
header:Two ways to implement complex MLP
The first direct method involves utilizing Cauchy-Riemann equations. 
The other method is to utilize and match a scalar potential function that generates our target data as first-order gradients. There, the Laplace equation is used as the main constraint.
Which one is better? The direct method is, and this gives us some numerical lessons about solving the Laplace equation as well.
Being updated. Laplace and Cauchy-Riemann connections.
header:Slight digression:neural networks work as lowpass interpolators and bounded lowpass extrapolators
In a significantly bounded way, due to the F-principle of neural networks where it takes almost exponential time for neural nets to learn high-frequency signals to the point that they never do, high-frequency signals tend to be filtered out.
This means that in some cases, even without complexifications, neural nets can be used for significantly bounded extrapolation. We can see this in the below image.
img:imgs/naive_extrapolation.png
The standard MLP (without even residual connections) of width 10 and depth 10 and tanh activation function with 5000 training points accompanied by first-order derivatives on [-25,25] was used.
Extrapolation to [-50,50] was successful. So first-order derivatives were sufficient for some extrapolation.
Unfortunately, this is a relatively lucky case and in other cases this miracle is more constrained, though there are some limited extrapolation successes.



