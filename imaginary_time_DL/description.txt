header:History (dates-wise)
v1 code (July 10-11, 2025)
Updating further soon
header:Codes
The initial v1 code can be viewed from the below repo viewer, located at codes/extrapolation.py. (July 10-11, 2025)
header:On the [-1,1] "constraint" of neural networks - is extrapolation on neural networks really possible?
Assume, for simplification, one-dimensional input domain.
If you think about it, if we can extract information well on [-1,1], then as long as the interpolating function is smooth, we know everything.
But neural networks do not behave like this. Outside of the interpolating domain where sample data are available, they behave poorly.
This is largely due to the nature of activation functions.
The worst case is RELU activation, where we high-order derivatives are all zero. So neural networks with RELU are essentially interpolating memorizers.
No RELU for extrapolation.
But what about other smooth activation functions? Can we do well by obtaining high-order derivative data?
Problem: high-order differentiation of neural nets is quite expensive. And we are doing additional differentiations on the loss function for derivative matching.
You may consider finite difference methods, but they are bound to different numerical issues.
Neural nets can only approximate some smooth function, so there will be some cases where derivatives are significantly wrong for correct computation of high-order derivatives.
So we need different ways to have good extrapolation behaviors. Expanding the sample input domain is not an option, as aforementioned.
header:Why not polynomials for extrapolation?
Polynomials do provide good bounded extrapolation performance for sufficiently low-derivative signals.
Nevertheless, they unnecessarily lose valuable high-order derivative information. Furthermore, it is not like we cannot extrapolate forever.
And there are cases where polynomials are clearly unsuited, especially in high-dimensional input high-dimensional output cases, along with non-linear optimization cases.
header:How do we "preserve" high-order derivative information of a signal?
Directly providing high-order derivatives so that neural networks match to them is infeasible.
Instead, we work in complex-valued domain and expand to complex-valued output. We move away from real analysis to complex analysis.
header:Holomorphic function
If we assume that our signals are holomorphic, then a significantly constraining constraint can be provided - Cauchy-Riemann equations.
So our neural nets evaluate and punish Cauchy-Rimeann equation residuals so that neural networks properly learn smooth interpolation and extrapolation.
This is much better than penalizing neural nets for non-smoothness measures directly. It is samples that tell us how to be (more) smooth (or not), not interpolated results.
header:Two ways to implement complex-variable physics-informed MLP neural network (complex PINN) 
The first direct method involves utilizing Cauchy-Riemann equations. ($\textbf{DirectUVMLP}$)
The other method is to utilize and match a scalar potential function that generates our target data as first-order gradients. There, the Laplace equation is used as the main constraint. ($\textbf{ScalarPotentialMLP}$)
Which one is better? The direct method is, and this gives us some numerical lessons about solving the Laplace equation as well.
header:DirectUVMLP
This model computes training data $f_{\text{target}}(\mathbf{z}) = (u_{\text{true}}(z),v_{\text{true}}(z))$, which has complex-valued input $z=(x,y)$ and complex-valued output.
"UV" in DirectUVMLP refers to the output being complex-valued, with $u$ being the real part of the output and $v$ being the imaginary part.
For holomorhpic functions, we have Cauchy-Riemann constraints, which state that:
$$\frac{\partial u_{\text{true}}}{\partial x} = \frac{\partial v_{\text{true}}}{\partial y},\,\, \frac{\partial u_{\text{true}}}{\partial y} = -\frac{\partial v_{\text{true}}}{\partial x}$$
This informs us the Cauchy-Riemann penalty to be used as part of the loss function.
Non-physics (non-Cauchy-Riemann) training data MSE loss goes as:
$$\mathcal{L}_{\text{uv\_data}} = \frac{1}{N} \sum_{i=1}^{N} \left\| f_\theta(\mathbf{z}_i) - (u_{\text{true}}(\mathbf{z}_i), v_{\text{true}}(\mathbf{z}_i)) \right\|^2$$
The Cauchy-Riemann penalty goes as:
$$\mathcal{L}_{\text{cr}} = \frac{1}{M} \sum_{j=1}^{M} \left[ \left( \frac{\partial \hat{u}}{\partial x} - \frac{\partial \hat{v}}{\partial y} \right)^2 + \left( \frac{\partial \hat{u}}{\partial y} + \frac{\partial \hat{v}}{\partial x} \right)^2 \right]$$
The optional kink penalty that penalizes varying Cauchy-Riemann residuals (so that we do not see sudden kinks as we move away from the interpolation domain) goes as:
$$\mathcal{L}_{\text{kink}} = \frac{1}{M} \sum_{j=1}^{M} \left\| \nabla R_{\text{CR}}(\mathbf{z}_j) \right\|^2$$
These losses are then weighted, either by fixed weights or by dynamic weighting. By default, dynamic weighting is used, to be described below.
header:ScalarPotentialMLP
This model computes scalar real-valued training data $\psi_{\text{target}}(\mathbf{z})$ from the available target function $f_{\text{target}}(\mathbf{z})$ and mainly fits this potential $\psi_{\text{target}}$ instead.
From the DirectUVMLP case, define as follows:
$$u_{\text{true}} = \frac{\partial \psi_{\text{target}}}{\partial x},\,\,v_{\text{true}} = - \frac{\psi_{\text{target}}}{\partial y}$$
Then one of the Cauchy-Riemann equations is automatically satisfied, so we only need to impose the other, which gives us the Laplace equation:
$$\nabla^2 \psi_{\text{target}} = 0$$
The potential data MSE loss goes as:
$$\mathcal{L}_{\psi\_\text{data}} = \frac{1}{N} \sum_{i=1}^{N} \left( \hat{\psi}(\mathbf{z}_i) - \psi_{\text{true}}(\mathbf{z}_i) \right)^2$$
The Laplace equation penalty goes as:
$$\mathcal{L}_{\text{laplace}} = \frac{1}{M} \sum_{j=1}^{M} \left( \nabla^2 \hat{\psi}(\mathbf{z}_j) \right)^2 = \frac{1}{M} \sum_{j=1}^{M} \left( \frac{\partial^2 \hat{\psi}}{\partial x^2} + \frac{\partial^2 \hat{\psi}}{\partial y^2} \right)^2$$
Despite equivalence for the target functions, neural network training works differently, and automatic satisfaction of $\frac{\partial u}{\partial y} = -\frac{\partial v}{\partial x}$ actually kills a valuable training signal that can be used to make neural nets converge to correct holomorphic solutions.
header:Dynamic weighting in training both complex MLPs
The dynamic weighting algorithm aims to automatically balance the contributions of different loss terms during optimization, adjusting the weight of each physics-based loss term at every training step to match the gradient magnitude of the data-driven loss terms. This prevents any single loss term from dominating the gradient updates, ensuring a more stable and balanced training process. (see https://doi.org/10.1016/j.cma.2022.114823 )
For a given set of model parameters $\theta$, the gradient of the loss function $\mathcal{L}_k$ is computed for every term $k$ in the total loss (both data and physics terms). The gradient $\mathbf{g}_k$ is a vector containing the partial derivatives of the loss with respect to each model parameter.
$$\mathbf{g}_k = \nabla_\theta \mathcal{L}_k(\theta)$$
A reference gradient magnitude is established using only the data-driven loss terms (e.g., $\mathcal{L}_{\text{uv\_data}}$, $\mathcal{L}_{\psi\_\text{data}}$). We compute the maximum absolute value of the gradient for each data loss term and then take the mean of these maximums. This value, $\lambda_{\text{ref}}$, represents the typical gradient scale produced by the data. This calculation is excluded from the gradient tape for the main optimization step, meaning its own derivative is not computed.
$$\lambda_{\text{ref}} = \underset{k \in \text{data\_losses}}{\text{mean}} \left( \max(|\mathbf{g}_k|) \right)$$
For each physics-based loss term $j$ (e.g., $\mathcal{L}_{\text{laplace}}$, $\mathcal{L}_{\text{cr}}$, $\mathcal{L}_{\text{kink}}$), a dynamic weight $w_j$ is calculated.
The gradient statistic for the physics loss, $\lambda_j$, is computed in the same way as the reference:
$$\lambda_j = \max(|\mathbf{g}_j|)$$
A raw weight, $\hat{w}_j$, is calculated as the ratio of the reference statistic to the physics statistic. This raw weight aims to rescale the physics gradient to the same order of magnitude as the data gradients.
$$\hat{w}_j = \frac{\lambda_{\text{ref}}}{\lambda_j + \epsilon}$$
where $\epsilon$ is a small constant (e.g., $10^{-8}$) for numerical stability.
To prevent erratic fluctuations between training steps, the final weight $w_j$ is updated using an Exponential Moving Average (EMA). This smooths the weight over time, blending the newly calculated raw weight with the weight from the previous step, $w_j^{\text{prev}}$.
$$w_j = \alpha \cdot w_j^{\text{prev}} + (1 - \alpha) \cdot \hat{w}_j$$
Here, $\alpha$ is the EMA decay rate, a hyperparameter typically close to 1 (e.g., 0.99).
The final gradient, $\mathbf{g}_{\text{total}}$, used to update the model parameters $\theta$ is a sum of the unweighted data gradients and the dynamically weighted physics gradients.
$$\mathbf{g}_{\text{total}} = \sum_{k \in \text{data\_losses}} \mathbf{g}_k + \sum_{j \in \text{physics\_losses}} w_j \cdot \mathbf{g}_j$$
This combined gradient is then passed to the optimizer (e.g., Adam) to perform the parameter update.
header:mini-batch SGD theoretical justifications of the circling explore-consolidate training approach
The question of how gradient descent finds global minima and its convergence property is not discussed - since this is just the usual MLP, and residual connections can easily be added if needed, the standard established understanding follows.
The question of the approximation capacity of a neural net is another issue - results for RELU activation functions are well-known with some of the best bounds, but for other activation functions, we only have limited theoretical knowledge. Here we assume that RELU is well-approximated by smooth activation functions such that we can leverage on univeral approximation results based on RELU.
For utilizing gradient descent results, we assume that mini-batch SGD used in this paper approximate gradient descent well enough such that global minima as well as convergence properties can be used.
In case of a sufficiently over-parameterized neural network, we can use NTK analysis to justify asymptotic convergence behaviors as well.
The circling explore-consolidate approach used here is essentially the mini-batch SGD but crafted such that learning does not have to learn the entire extrapolation domain at once, which creates instability.
Instead, we focus on the $[-1,1]^2$ interpolation complex domain and then move from nearby 2D boxees finally to the boundary of the full extrapolation domain.
When we cover the interpolation domain one box at a time, only non-physics data points within the box are explored, while physics constraint points are sampled from the entire extrapolation domain.
When we complete covering the entire interpolation domain for non-physics data points, non-physics data points are sampled from the entire interpolation domain, but physics constraint points are sampled only from the box (outside the interpolation domain) currently being explored.
Consolidation phases consider all the boxes so far explored, re-examine gradients involved and update the net.
img:imgs/training_phases.png
Anchors refer to additional points sampled outside the interpolation domain but inside the extrapolation domain.
header:A more elaborate description of the circling explore-consolidate curriculum training
The Circling Explore-Consolidate (CEC) approach is a structured, spatiotemporal training curriculum designed to enhance the stability, accuracy, and extrapolation capabilities of physics-informed neural networks. It addresses the fundamental challenge of training a model over a large and complex domain by decomposing the problem into a progressive sequence of focused learning and global reconciliation stages. The "circling" methodology provides a geometrically and physically motivated strategy for this decomposition.
The approach is founded on two core principles: **Explore-Consolidate Learning** and a **Concentric Expansion Strategy**.
1. The Core Principle: Explore-Consolidate
The explore-consolidate paradigm is a curriculum learning technique designed to mitigate the problem of catastrophic forgetting, where a neural network abruptly loses knowledge of a previously learned task when trained on a new one. In the context of a spatial domain, this translates to the model losing its global solution integrity when focusing on a new, localized region. Catastrophic forgetting is also partially mitigated by repeating the entire training process again.
**Explore Phase:** The model's training is focused on a small, novel subdomain. During this phase, the loss function is computed predominantly or exclusively on training points sampled from this new region. This allows the network to rapidly adapt its parameters to satisfy the local data-fitting or physics-based objectives without the immediate, competing influence of the full domain.
**Consolidate Phase:** Following one or more exploration steps, a consolidation phase is initiated. The training dataset is expanded to include points from the union of all previously explored subdomains. By training on this cumulative dataset, the model is forced to reconcile its newly acquired local knowledge with its existing global solution. This "rehearsal" step integrates the new information and ensures the stability and coherence of the function approximation across the entire known domain.
2. The Emphasis: The "Circling" Expansion Strategy
The "circling" aspect of the curriculum defines the specific geometric sequence of the subdomains used for exploration. Instead of a simple raster scan (e.g., left-to-right) or random decomposition, the CEC approach expands the training domain in a series of concentric, square-like annuli (or shells), moving from a central region outwards.
Other than learning the interpolation domain first, we could say that this outward expansion is motivated by the mathematical properties of the physical systems being modeled, such as those governed by elliptic partial differential equations (e.g., Laplace's equation, which is fundamental to the Cauchy-Riemann equations), but this is not too critically important, though we can add technical justifications:
A. Information Propagation from Boundaries: For many physical systems, the solution within a domain is uniquely determined by the conditions on its boundary (a well-posed boundary value problem). The circling strategy mimics this property. By first establishing a robust solution in a central region (e.g., the data domain from `[-1, 1]^2`), the model creates a trusted "inner boundary." Each subsequent exploration phase learns to satisfy the physics in a new shell, using the edge of the previously consolidated region as an implicit, learned boundary condition. This allows information to propagate outward in a structured and stable manner.
B. Progressive Domain Expansion: The model is never forced to make a large, uninformed leap into a distant, unknown region. The domain of physical validity is expanded gradually and contiguously. This incremental process is more stable than simultaneously enforcing constraints over a vast domain, which can lead to conflicting gradients and poor convergence in the early stages of training.
C. Hierarchical Feature Learning: The concentric expansion encourages the network to learn a hierarchical representation, with expanding details.
The curriculum is implemented as a two-stage process:
Stage 1: Data-Fitting Curriculum:
**Explore:** The model starts by learning the data fit in a small box at the origin.
**Circle & Explore:** It then explores subsequent data points in concentric shells, progressively expanding until the entire data domain (e.g., `[-1, 1]^2`) is covered.
**Consolidate:** After each shell is fully explored, a consolidation phase runs on all data points seen so far to ensure a unified fit. Throughout this stage, physics constraints may be loosely applied across a wider domain to guide the solution towards a physically plausible one.
Stage 2: Physics-Finetuning Curriculum:
**Explore:** With a solid fit on the data domain, the curriculum begins to explore the extrapolation region. It samples physics-based constraint points from the first shell immediately outside the data domain. During this phase, loss from the original data domain is also included to prevent the fit from drifting.
**Circle & Explore:** The process continues, exploring ever-larger concentric shells in the extrapolation domain.
**Consolidate:** After exploring each new physics shell, a consolidation phase runs on all physics regions explored so far, plus the full data domain, to integrate the new knowledge and maintain the global solution's integrity.
To summarize, the Circling Explore-Consolidate curriculum is a sophisticated training methodology that structures learning both temporally (explore then consolidate) and spatially (in concentric shells) so that instability in learning a large input domain can be avoided.
header:Why complexification is powerful
Complexification is powerful because it effectively provides high-order derivative and frequency information.
Even when we only have real-valued real-input data, it may be that we have additional information about their frequency details, such as the data being low-frequency data within some bounded frequency domain. Constructing at least an approximate holomorphic function modeling these data allows us to encode this information and use it for further training and extrapolating from neural nets.
We can also think of Cauchy's integral formula - for closed disk $D$ contained in some open subset $U$ and for every complex-valued $a$ on the interior of $D$,
$$D = \bigl\{z:|z - z_0| \leq r\bigr\}$$
$$f(a) = \frac{1}{2\pi i} \oint_\gamma \frac{f(z)}{z-a}\,dz$$
$$f^{(n)}(a) = \frac{n!}{2\pi i} \oint_\gamma \frac{f(z)}{\left(z-a\right)^{n+1}}\,dz$$
where $\gamma$ is the circle oriented counter-clockwise forming the boundary of $D$ and $f:U \rightarrow \mathbb{C}$ is assumed to be holomorphic. Furthermore, for holomorphic functions, if some function $g(x) = f(x)$ for every real-axis input within $D$, then $g(x)=f(x)$ everywhere.
header:Slight digression: neural networks work as lowpass interpolators and bounded lowpass extrapolators
In a significantly bounded way, due to the F-principle of neural networks where it takes almost exponential time for neural nets to learn high-frequency signals to the point that they never do, high-frequency signals tend to be filtered out.
This means that in some cases, even without complexifications, neural nets can be used for significantly bounded extrapolation. We can see this in the below image.
img:imgs/naive_extrapolation.png
The standard MLP (without even residual connections) of width 10 and depth 10 and tanh activation function with 5000 training points accompanied by first-order derivatives on [-25,25] was used.
Extrapolation to [-50,50] was successful. So first-order derivatives were sufficient for some extrapolation.
Unfortunately, this is a relatively lucky case and in other cases this miracle is more constrained, though there are some limited extrapolation successes.
So we need complexification.
header:Some empirical training insights
1. For most cases, the kink penalty significantly improves training performance. The kink penalty is well-justified since the Cauchy-Riemann residuals should be zero so their high-order and first-order derivatives should always be zero.
